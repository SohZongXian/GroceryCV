{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de29b18e-6876-4f2c-a5b1-57d306e2e6d8",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "46e2b123-c1a4-451f-a202-cddd059ba14f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5fd2f67-6626-4353-aa87-556d1353d4ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Image preprocessing/class classification\n",
    "classes = []\n",
    "\n",
    "class_dir = \"C:/Users/user/Documents/GitHub/GroceryCV/GroceryStoreDataset-master/dataset/train/\"\n",
    "\n",
    "for i, classname in enumerate(os.listdir(class_dir)):\n",
    "    print(classname)\n",
    "    classes.append(classname)\n",
    "\n",
    "\n",
    "os.getcwd()\n",
    "for c in classes:\n",
    "    collection = \"C:/Users/user/Documents/GitHub/GroceryCV/GroceryStoreDataset-master/dataset/train/\" + c\n",
    "    for i, filename in enumerate(os.listdir(collection)):\n",
    "        if(collection+\"/\"+ c + \"_\" + str(i) + \".jpg\" == collection+\"/\"+filename or collection+\"/\"+ c + \"_0\" + str(i) + \".jpg\" == collection+\"/\"+filename):\n",
    "            print(collection+\"/\"+filename)\n",
    "            continue\n",
    "        if(i<10):\n",
    "            os.rename(\"C:/Users/user/Documents/GitHub/GroceryCV/GroceryStoreDataset-master/dataset/train/\" + c + \"/\" + filename, \"C:/Users/user/Documents/GitHub/GroceryCV/GroceryStoreDataset-master/dataset/train/\" + c + \"/\" + c + \"_000\" + str(i) + \".jpg\")\n",
    "        else:\n",
    "            os.rename(\"C:/Users/user/Documents/GitHub/GroceryCV/GroceryStoreDataset-master/dataset/train/\" + c + \"/\" + filename, \"C:/Users/user/Documents/GitHub/GroceryCV/GroceryStoreDataset-master/dataset/train/\" + c + \"/\" + c + \"_00\" + str(i) + \".jpg\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d7a5a2a1-4950-446a-a276-0635a233df58",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "##image aug datagen\n",
    "datagen_aug = ImageDataGenerator(rotation_range=40,\n",
    "                             rescale = 1./255,\n",
    "                             width_shift_range=0.2,\n",
    "                            height_shift_range=0.2,\n",
    "                            shear_range=0.2,\n",
    "                            zoom_range=0.2,\n",
    "                            horizontal_flip=True\n",
    "                           )\n",
    "\n",
    "##normal datagen\n",
    "datagen = ImageDataGenerator(rescale=1./255)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b201622d-52f3-4ee3-aa56-5ec0363068c9",
   "metadata": {},
   "source": [
    "## Loading image with augmentation (Less data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "53c37e93-2d10-4804-b384-f4c6feb4f9bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "##augmented datagen\n",
    "training_set_aug = datagen_aug.flow_from_directory('C:/Users/user/Documents/GitHub/GroceryCV/GroceryStoreDataset-master/dataset/train_less_data',\n",
    "                                          batch_size=32,\n",
    "                                           target_size = (64,64),\n",
    "                                          class_mode='binary')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fbd12bd-f239-4cea-b14c-b0b0163b5cfe",
   "metadata": {},
   "source": [
    "## Loading image without augmentation (Less data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "58abe4b2-03c2-4c35-adff-e6b90360c641",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 513 images belonging to 34 classes.\n"
     ]
    }
   ],
   "source": [
    "##non augmented datagen\n",
    "training_set = datagen.flow_from_directory('C:/Users/user/Documents/GitHub/GroceryCV/GroceryStoreDataset-master/dataset/train_less_data',\n",
    "                                          batch_size=32,\n",
    "                                           target_size = (64,64),\n",
    "                                          class_mode='binary')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ef2d98-04ca-404f-93a9-ca2163a7da22",
   "metadata": {},
   "source": [
    "## Load test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "86a5547c-c5d0-4538-b8af-d9f3dfcfb239",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 198 images belonging to 34 classes.\n"
     ]
    }
   ],
   "source": [
    "test_datagen = ImageDataGenerator(rescale = 1./255)\n",
    "test_set = test_datagen.flow_from_directory('C:/Users/user/Documents/GitHub/GroceryCV/GroceryStoreDataset-master/dataset/test',\n",
    "                                          batch_size=32,\n",
    "                                        target_size = (64,64),\n",
    "                                          class_mode='binary')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed9045be-65dc-45c1-9486-ac38ca4ee219",
   "metadata": {},
   "source": [
    "## Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1f792bf8-f447-4d0a-a987-34c447f3ce20",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'adam' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-2c87ae60fcb3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m                            ])\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m model.compile(optimizer = adam(lr=0.001, beta_1=0.9, beta_2=0.999),\n\u001b[0m\u001b[0;32m     21\u001b[0m               \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'categorical_crossentropy'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m               metrics = ['accuracy'])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'adam' is not defined"
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Conv2D(64, (3,3), padding = 'same', activation='relu', input_shape=(64,64,3)),\n",
    "    tf.keras.layers.MaxPooling2D(2, 2),\n",
    "    tf.keras.layers.Dropout(rate=0.2),\n",
    "    tf.keras.layers.Conv2D(64, (3,3), padding = 'same', activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    tf.keras.layers.Dropout(rate=0.2),\n",
    "    tf.keras.layers.Conv2D(128, (3,3), padding = 'same', activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    tf.keras.layers.Dropout(rate=0.2),\n",
    "    tf.keras.layers.Conv2D(128, (3,3), padding = 'same', activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    tf.keras.layers.Dropout(rate=0.2),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(256, activation='relu'),\n",
    "    tf.keras.layers.Dropout(rate=0.5),\n",
    "    tf.keras.layers.Dense(10, activation='softmax')\n",
    "                           ])\n",
    "\n",
    "model.compile(optimizer = 'adam',\n",
    "              loss = 'categorical_crossentropy',\n",
    "              metrics = ['accuracy'])\n",
    "\n",
    "\n",
    "# learning_rate_reduction = ReduceLROnPlateau(monitor='val_acc', \n",
    "#                                             patience=3, \n",
    "#                                             verbose=1, \n",
    "#                                             factor=0.6, \n",
    "#                                             min_lr=0.00001)\n",
    "\n",
    "# batch_size = 64\n",
    "\n",
    "\n",
    "# history = model.fit(\n",
    "#             train_datagen.flow(X_train,y_train,batch_size = batch_size),\n",
    "#             epochs = 25,\n",
    "#             batch_size = batch_size,\n",
    "#             validation_data = (X_val,y_val),\n",
    "#             steps_per_epoch = X_train.shape[0]//batch_size,\n",
    "#             verbose = 1,\n",
    "#             callbacks=[learning_rate_reduction]\n",
    "#             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf8e9ca-d630-4e2b-a79e-ec2f5c433507",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ee1f31-45a1-4cbf-bbe8-fcd62a152df1",
   "metadata": {},
   "source": [
    "## Testing Manual image augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9a884801-7167-4141-b651-97e96da5efdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_dir = \"C:/Users/user/Documents/GitHub/GroceryCV/GroceryStoreDataset-master/dataset/test/Apple_Aug/Apple_01.jpg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "03df9700-94eb-4d56-9f2e-48be3a7a74af",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = load_img(img_dir)\n",
    "\n",
    "x=img_to_array(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9bb14104-eee9-4fcc-8f33-e06dc6897e18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(464, 348, 3)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a5f3efd1-bc12-408a-b491-8b349202b2d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = x.reshape((1,)+x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1dcbbc1b-e134-423f-93d0-2048921d6bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = \"C:/Users/user/Documents/GitHub/GroceryCV/GroceryStoreDataset-master/dataset/test/Apple_Aug\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb07db65-27c7-41bf-b106-0242eaeb1886",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "for batch in datagen.flow(x, batch_size=1, save_to_dir=save_dir, save_prefix='Apple_', save_format='jpeg'):\n",
    "    i+=1\n",
    "    if i>5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10782ad4-2daf-4ffe-abaf-ea768f63ebe4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
