{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b2d82fd-d6ea-4808-9116-627641b62b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install torch==1.9.0+cu102 torchvision==0.10.0+cu102 torchaudio===0.9.0 -f https://download.pytorch.org/whl/torch_stable.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40404852-e2bf-447e-baa4-908611737336",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load libraries\n",
    "import time\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import glob\n",
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "from torchvision.transforms import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import Adam\n",
    "from torch.autograd import Variable\n",
    "import torchvision\n",
    "import pathlib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a7a65a9-4be0-4844-8529-0743081aeb4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking for device\n",
    "device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1961f50d-2ef5-42d5-9512-7c832d464d10",
   "metadata": {},
   "source": [
    "## Augmentation object creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d58e1926-e005-48c5-87b5-fe60c7735982",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transforms\n",
    "transformer=transforms.Compose([\n",
    "    transforms.Resize((150,150)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),  #0-255 to 0-1, numpy to tensors\n",
    "    transforms.Normalize([0.5,0.5,0.5], # 0-1 to [-1,1] , formula (x-mean)/std\n",
    "                        [0.5,0.5,0.5])\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdbc9ad7-ac38-4800-8956-24cba296d718",
   "metadata": {},
   "source": [
    "## Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ec8a66-ba12-4ea7-a5d2-817a89e2024d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check image\n",
    "#Path for training and testing directory\n",
    "train_path='C:/Users/user/Documents/GitHub/GroceryCV/GroceryStoreDataset-master/dataset/train'\n",
    "test_path='C:/Users/user/Documents/GitHub/GroceryCV/GroceryStoreDataset-master/dataset/test'\n",
    "\n",
    "train_img =  torchvision.datasets.ImageFolder(root = train_path,transform=transformer)\n",
    "test_img = torchvision.datasets.ImageFolder(root = test_path,transform=transformer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2652310-c8c3-4fc5-a7cd-986abcaf2ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_transformed_image(dataset):\n",
    "    loader = torch.utils.data.DataLoader(dataset, batch_size = 6, shuffle=True)\n",
    "    batch = next(iter(loader))\n",
    "    images, labels = batch\n",
    "    \n",
    "    grid = torchvision.utils.make_grid(images, nrow = 3)\n",
    "    plt.figure(figsize = (11,11))\n",
    "    plt.imshow(np.transpose(grid, (1,2,0)))\n",
    "    print('Labels:', labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39483b1e-ab93-413b-8285-43675ab54e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_transformed_image(train_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a284801b-6722-43f4-a314-ca6bdd931492",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dataloader\n",
    "\n",
    "#Path for training and testing directory\n",
    "# train_path='C:/Users/user/Documents/GitHub/GroceryCV/GroceryStoreDataset-master/dataset/train'\n",
    "# test_path='C:/Users/user/Documents/GitHub/GroceryCV/GroceryStoreDataset-master/dataset/test'\n",
    "\n",
    "# train_loader=DataLoader(\n",
    "#     torchvision.datasets.ImageFolder(train_path,transform=transformer),\n",
    "#     batch_size=64, shuffle=True\n",
    "# )\n",
    "# test_loader=DataLoader(\n",
    "#     torchvision.datasets.ImageFolder(test_path,transform=transformer),\n",
    "#     batch_size=32, shuffle=True\n",
    "# )\n",
    "\n",
    "train_loader=DataLoader(\n",
    "    train_img,\n",
    "    batch_size=64, shuffle=True\n",
    ")\n",
    "test_loader=DataLoader(\n",
    "    test_img,\n",
    "    batch_size=32, shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e69fad86-71c3-41a3-be03-3e817195b043",
   "metadata": {},
   "outputs": [],
   "source": [
    "root=pathlib.Path(train_path)\n",
    "classes=sorted([j.name.split('/')[-1] for j in root.iterdir()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f43c7326-708b-4c39-acc8-706526905c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f10f43a-0544-41c1-a0ff-ae7376118b00",
   "metadata": {},
   "source": [
    "## Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a43e5cd-a562-41de-96c1-d90ca3c3491f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model=ConvNet(num_classes=len(classes)).to(device)\n",
    "model = models.resnet50()\n",
    "model.load_state_dict(torch.load('resnet50-0676ba61.pth'))\n",
    "\n",
    "for parameter in model.parameters():\n",
    "    parameter.requires_grad = False\n",
    "    \n",
    "model.fc = torch.nn.Linear(in_features=2048,out_features=len(classes),bias=True)\n",
    "\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31abf333-1d7e-444a-9d95-a33845992851",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Optmizer and loss function\n",
    "optimizer=Adam(model.parameters(),lr=0.001,weight_decay=0.0001)\n",
    "loss_function=nn.CrossEntropyLoss()\n",
    "num_epochs=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "643daa42-703a-4bd1-886b-598a3a89528c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_count=len(glob.glob(train_path+'/**/*.jpg'))\n",
    "test_count=len(glob.glob(test_path+'/**/*.jpg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d0788b1-00d8-4a23-b16d-1379fd86c81c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_count,test_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65aac053-84e0-4b88-9253-fb4830e8913f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()\n",
    "start = time.time()\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    #Evaluation and training on training dataset\n",
    "    model.train()\n",
    "    train_accuracy=0.0\n",
    "    train_loss=0.0\n",
    "    \n",
    "    for i, (images,labels) in enumerate(train_loader):\n",
    "        if torch.cuda.is_available():\n",
    "            images=Variable(images.cuda())\n",
    "            labels=Variable(labels.cuda())\n",
    "            \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs=model(images)\n",
    "        loss=loss_function(outputs,labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        \n",
    "        train_loss+= loss.cpu().data*images.size(0)\n",
    "        _,prediction=torch.max(outputs.data,1)\n",
    "        \n",
    "        train_accuracy+=int(torch.sum(prediction==labels.data))\n",
    "        \n",
    "    train_accuracy=train_accuracy/train_count\n",
    "    train_loss=train_loss/train_count\n",
    "    \n",
    "    \n",
    "    # Evaluation on testing dataset\n",
    "    model.eval()\n",
    "    \n",
    "    test_accuracy=0.0\n",
    "    for i, (images,labels) in enumerate(test_loader):\n",
    "        if torch.cuda.is_available():\n",
    "            images=Variable(images.cuda())\n",
    "            labels=Variable(labels.cuda())\n",
    "            \n",
    "        outputs=model(images)\n",
    "        _,prediction=torch.max(outputs.data,1)\n",
    "        test_accuracy+=int(torch.sum(prediction==labels.data))\n",
    "    \n",
    "    test_accuracy=test_accuracy/test_count\n",
    "    \n",
    "    \n",
    "    print('Epoch: '+str(epoch)+' Train Loss: '+str(train_loss)+' Train Accuracy: '+str(train_accuracy)+' Test Accuracy: '+str(test_accuracy))\n",
    "    \n",
    "end = time.time()\n",
    "print(\"Time taken:\", end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0db50f4-7218-4a60-a5ba-c93554f2c3df",
   "metadata": {},
   "source": [
    "## Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed945c37-5377-450d-a22d-e053999bc9d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, 'deployment/grocery_moreclass.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e79687d-4a70-46e2-9804-20b59cc70c5e",
   "metadata": {},
   "source": [
    "## Model Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c6ace56-098b-44b0-b608-768c915c9ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 \n",
    "from PIL import Image \n",
    "import copy\n",
    "\n",
    "img_file_path = 'sample_images/natural/Banana.jpg'\n",
    "\n",
    "#Transforms\n",
    "transformer_infer=transforms.Compose([\n",
    "    transforms.Resize((150,150)),\n",
    "    transforms.ToTensor(),  #0-255 to 0-1, numpy to tensors\n",
    "    transforms.Normalize([0.5,0.5,0.5], # 0-1 to [-1,1] , formula (x-mean)/std\n",
    "                        [0.5,0.5,0.5])\n",
    "])\n",
    "\n",
    "# m = torch.load('grocery_lessclass.model')\n",
    "m = torch.load('deployment/grocery_moreclass.pth')\n",
    "m.eval()\n",
    "\n",
    "def prediction(img_path, transformer):\n",
    "    image = Image.open(img_path)\n",
    "    image_tensor = transformer(image).float()\n",
    "    image_tensor = image_tensor.unsqueeze_(0)\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        image_tensor.cuda()\n",
    "        m.cuda()\n",
    "    \n",
    "    input = Variable(image_tensor)\n",
    "    output=m(input.to(device))\n",
    "    \n",
    "    index = output.cpu().data.numpy().argmax()\n",
    "    pred = classes[index]\n",
    "    return pred\n",
    "\n",
    "prediction(img_file_path, transformer_infer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a658358b-b7b0-4be0-aaf7-2defb4a2b593",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
